%
% $Id: intro.tex 2011-05-03 21:42:00Z by KBJ$
%
%\documentclass[12pt]{article}
%\usepackage{CJK}
%\usepackage{pinyin}




\begin{CJK}{Bg5}{bsmi}
\chapter{Introduction}
\label{sec:intro}

Lattice-based cryptography is a hot topic, with numerous submissions
and publications at prestigious conferences in the last two years.
%
The reasons that it might have become so popular include:
\begin{itemize}
\item lattice-based PKCs, unlike ECC, do not immediately succumb to
  large quantum computers (i.e., they are ``post-quantum.'');
\item lattice-based PKCs enjoy the (so far) unique property of being
  protected by a worst-case hardness assumption (i.e., they are
  unbreakable if \emph{any} of a large class of lattice-based problem
  \emph{at a lower dimension} is intractible)\cite{Ajtai:97, Micciancio:Regev:07};
\item lattices is used to create the first fully homomorphic encryptions\cite{Gentry:09}.
\end{itemize}


One of the main problems in lattice-based cryptography is the
\emph{shortest vector problem} (SVP).
%
As the name implies, it is a search for a non-zero vector with the
smallest Euclidean norm in a lattice.
%
The SVP is NP-hard under randomized reductions.
%
The \emph{approximate shortest vector problem} (ASVP) is the search
for a short non-zero vector whose length is at most some given
multiple of the minimum.
%
It is easy in some cases, as shown by the LLL algorithm~\cite{LLL:1982}.
%
Although LLL has polynomial running time, the approximation factor of
LLL is exponential in the lattice dimension.
%
The complexity of SVP (and ASVP) has been studied for decades, but
practical implementations that take advantage of special hardware are
not investigated seriously until
recently~\cite{DBLP:conf/africacrypt/HermansSBVP10,DagdelenS10,DetreyHPS10}.

In contrast, enumeration is another way to solve SVP and ASVP, which
can be viewed as a depth-first search in a tree structure, going over
all vectors in a specified search region deterministically.
%
Typically, a basis transformation such as BKZ algorithm is performed first to
improve the basis to one likely to yield a short vector via
enumeration~\cite{SchnorrE1994}.

At Eurocrypt 2010, Gama~\emph{et~al.}\ proposed the \emph{Extreme
  Pruning} approach to solving SVP and ASVP~\cite{GamaNR10} and showed
that it is possible to speed up the enumeration exponentially by
randomizing the algorithm.
%
The idea is that, instead of spending a lot of time searching
\emph{one} tree, one generates \emph{many} trees and only spends a
small amount of time on each of them by aggressively pruning the
subtrees unlikely to yield short vectors using a bounding function.
%
That is, one focuses on the parts of the trees that are more
``fruitful'' in terms of the likelihood of producing short vectors per
unit time spent.

In other words, one should try to maximize the success probability of
finding a short vector \emph{per unit of computing time spent} by
choosing an appropriate bounding function in pruning.
%
Therefore, which bounding function works better depends on the
particular \emph{implementation}.

\begin{comment}
In this paper, we make a practical contribution on several fronts.
\begin{enumerate}
\item We use a polynomial bounding function, which performs better
  than the linear bounding function analyzed in Gama \emph{et
    al}.~\cite{GamaNR10}\ \
\item We recursively apply this brilliant idea of extreme pruning to
  BKZ so that we can quickly generate many bases of higher quality,
  which leads to better higher probability of finding short vectors
  than reported by Gama \emph{et al}.
\item We harness the immense computational power of graphic cards to
  speed up the computation.
\item Last but not least, we have run our code on Amazon's EC2, as
  such (A)SVP computations are seen to be easily adaptable to such
  cloud computing services.
\end{enumerate}
As a result, the ``cost'' of breaking a lattice-based cryptosystem can
henceforth be measured directly in U.S.\ dollars, taking Lenstra's
\emph{dollarday} metric~\cite{Lenstra2004KeyLength} to a next level.
%
That is, the cost will be shown literally as an amount on your
invoice, e.g., the effort in our solving a 114-dimensional instance of
the SVP challenge translating to a US\$400 bill from Amazon.
%
Moreover, this new metric is more practical in that the
parallelizability of the algorithm or the parallelization of the
implementation is \emph{explicitly} taken into account, as opposed to
being assumed or unspecified in the dollarday metric.
\end{comment}


\section{Motivation}
Although lots of study on the subject of lattce-base cryptanalysis, the pratical security level remains unknown. Specifically, there is no exact and reasonable method to estimate the computational power for a SVP instance. Thus, we parallelize the fast lattice enumeration algorithm on GPU and in clouds with high performance implementation. Through this way, we can estimate the security level of those lattice-base cryptosystem.
\\


\section{Preliminaries}

Let $m,n \in \ZZ$ with $n\leq m$, and let $\vec{b}_{i} \in \ZZ^{m}$ for $1\leq i \leq n$ be a set of linearly independent vectors. The set of all integer linear combinations of the vectors $\vec{b}_{i}$ is called a lattice  $\mathpzc{L}$:
\begin{align*}
 \mathpzc{L} = \left\{ \sum_{i=1}^{n} x_{i} \vec{b}_{i} \; | \; x_{i} \in \ZZ \right\} \,.
\end{align*}

The matrix $\mat{B}\in \ZZ^{m\times n}$ consisting of the column vectors $\vec{b}_{i}$ is called a basis of  $\mathpzc{L}$, we write  $\mathpzc{L} = \latticeb{B}$ to denote the lattice spanned by basis $\mat{B}$.
 $\mathpzc{L}$ is an additive group in $\ZZ^{m}$. If $n=m$ the lattice is called full-dimensional.
The basis of a lattice is not unique. The product of a basis with an unimodular matrix $\mat{M}$ ($\abs{\det(\mat{M})} = 1$) does not change the lattice, that is,  $\latticeb{B} = \latticeb{MB}$.

The value $\lambda_{1}(\latticeb{B})$ denotes the norm of a shortest non-zero vector in the lattice. It is called the first successive minimum. The determinant of a lattice is the value $\det(\latticeb{B}) = \sqrt{\det(\mat{B}^{t}\mat{B})}$. If $\latticeb{B}$ is full-dimensional, then the lattice determinant is equal to the absolute value of the determinant of the basis matrix, i.e. $\det(\latticeb{B}) = \abs{\det(\mat{B})}$. Note that, the determinant of a lattice is independent of the basis; if the basis changes, the determinant remains the same. In the remainder of this paper, we will only be concerned with full-dimensional lattices.

% todo : solve the mu problem 

The shortest vector problem in lattices is stated as follows. Given a lattice basis $\mat{B}$, output a vector $\vec{v} \in \latticeb{B} \setminus \{0\}$ subject to $\norm{\vec{v}} = \lambda_{1}(\latticeb{B})$.

The Gaussian heuristic assumes that the number of lattice points inside a set $S$ is approximately $\text{vol}(S) / \text{vol}(\mathpzc{L})$. Using this heuristic and the volume of a unit sphere in dimension $n$, we can compute an approximation of the first minimum of the lattice $\mathpzc{L}$:
%
$\FM(\mathpzc{L}) =  \frac{\Gamma(n/2+1)^{1/n}}{\sqrt{\pi}} \cdot \det(\mathpzc{L})^{1/n}\,.$
Here $\Gamma(x)$ is the gamma-function.
%--
This estimate is used, among others, to predict the length of shortest vectors in the SVP challenge \cite{svpchallenge}. In our experiments as well as in the SVP challenge the heuristic shows to be a good estimate of a shortest vector length for the lattices used. Throughout the rest of this paper, our goal will always be to find a vector below $1.05 \cdot \FM(\mathpzc{L})$, the same as in the SVP challenge.

The Gram-Schmidt orthogonalization (GSO) of a matrix $\mat{B} \in \ZZ^{\dim \times \dim}$ is
$\mat{B}^{*} = [\vec{b}_{1}^{*}, \ldots, \vec{b}_{\dim}^{*}] \in \RR^{\dim \times \dim}$. It is computed via $\vec{b}_{i}^* = \vec{b}_{i}- \sum_{j=1}^{i-1}\mu_{i,j}\vec{b}^*_{j}$ for $i = 1,\ldots , n$, where
$\mu_{i,j} = \vec{b}_{i}^T \vec{b}_{j}^* / \norm{\vec{b}_{j}^*}^2 \text{ for all } 1~\le~j~\le~i~\le~\dim$.
%--
We have $\mat{B} = \mat{B}^* \, \mu^{T}$, where $\mat{B}^*$ is orthogonal and $\mu^{T}$ is an upper triangular matrix. Note that $\mat{B}^{*}$ is not necessarily a lattice basis. The values $\mu$ are called the Gram-Schmidt coefficients.



The LLL \cite{LLL:1982} and the BKZ \cite{SchnorrE1994} algorithms can be used for pre-reduction of lattices, before running an SVP algorithm. Pre-reduction speeds up the enumeration, since the size of the enumeration tree is depending on the quality of the input basis. BKZ is controlled by a blocksize parameter $\beta$, and LLL is the special case of BKZ with parameter $\beta = 2$. Higher blocksize guarantees a better reduction quality, in the sense that vectors in the basis are shorter and the angles between basis vectors are closer to orthogonal. The gain in reduction quality comes at the cost of increasing runtime. The runtime of BKZ increases exponentially with the blocksize $\beta$. In the lattice dimension, the runtime of BKZ behaves polynomial in practice, whereas no proof of this runtime is known. The overall runtime of our SVP solver will include the BKZ pre-reduction runtimes as well as enumeration runtimes. It is an important issue to find suitable blocksize parameters for pre-reduction.



%====================
\section{Cloud Computing, Amazon EC2, and GPU}
%====================

Cloud computing is an emerging computing paradigm that allows data
centers to provide large-scale computational and data-processing power
to the users on a ``pay-as-you-go'' basis.
%
Amazon Web Services (AWS) is one of the earliest and major
cloud-computing providers, who provides, as the name suggests, web
services platforms in the cloud.
%
The Elastic Compute Cloud (EC2) provides compute capacity in the cloud
as a foundation for the other products that AWS provides.
%
With EC2, the users can rent large-scale computational power on demand
in the form of ``instances'' of virtual machines of various sizes,
which is charged on an hourly basis.
%
The users can also use popular parallel computing paradigms such as
the MapReduce framework~\cite{Dean2004MapReduce}, which is readily
available as the AWS product ``Elastic MapReduce.''
%
Furthermore, such a centralized approach also frees the users from the
burden of provisioning, acquiring, deploying, and maintaining their
own physical compute facilities.

Naturally, such a paradigm is economically very attractive for most
users, who only need large-scale compute capacity occasionally.
%--
For large-scale computations, it may be advisable to buy machines
instead of renting them because Amazon presumably expects to make a
profit on renting out equipment, so our extrapolation might
over-estimate the cost for long-term computations.  
%
However, we believe that these cloud-computing service providers will
become more efficient in the years to come if cloud computing indeed
becomes the mainstream paradigm of computing.
%
Moreover, trade
rumors has it that Amazon's profit margins are around 0\% (break-even)
as of mid-2011, and nowhere close to 100\%, so we can say confidently
that Amazon rent cannot be more than $2\times$ what a
large-scale user would have spent if he bought and maintained his own
computers and networking.  Thus, Amazon prices can still be considered
a realistic measure of computing cost and a good yardstick for
determining the strength of cryptographic keys.


In estimating complexity such that of solving (A)SVP or problems of
the same or similar nature, Amazon EC2 can be used to provide a common
measure of cost as a metric in comparing alternative or competing
cryptanalysis algorithms and their implementations.
%
Moreover, when using the Amazon EC2 metric, the parallelizability of
the algorithm or the parallelization of the implementation
is \emph{explicitly} taken into account, as opposed to being assumed
or unspecified.
%
In addition to its simplicity, we argue that the EC2 metric is more
practical than the \emph{dollardays}
metric of \cite{Lenstra2004KeyLength}, and a recent report by
% computation of the 172 days: 2,10*h+0.42*h < 5630+0.74*h+0.42*h
Kleinjung, Lenstra, Page, and Smart~\cite{KleinjungLPS11} also agrees with us in taking a
similar approach and measure with Amazon's EC2 cloud.


Graphics processing units (GPUs) represent another class of many-core
architectures that are cost-effective for achieving high arithmetic
throughput.
%
The success of GPU has mainly been driven by the economy of scale in
the video game industry.
%
Currently, the most widely used GPU development toolchain is NVIDIA's
CUDA (Compute Unified Device Architecture)~\cite{Kirk2010CUDA}.
%
At the core of CUDA are three key abstractions, namely, a hierarchy of
thread groups, shared memories, and barrier synchronization, that are
exposed to the programmers as a set of extensions to the C programming
language.
%
At the system level, the GPU is used as a coprocessor to the host
processor for massively data-parallel computations, each of which is
executed by a grid of GPU threads that must run the same program (the
kernel).
%
This is the SPMD (single program, multiple data) programming model,
similar to SIMD but with more flexibility such as in changing of data
size on a per-kernel-launch basis, as well as deviation from SIMD to
MIMD at a performance penalty.

AWS offers several different compute instances for their customers to
choose based on their computational needs.
%
The one that interests us the most is the largest instance called
``Cluster Compute Quadruple Extra Large'' (\texttt{cc1.4xlarge})
which is designed for high-performance computing.
%
Each such instance consists of 23~GB memory provide 33.5 ``EC2 Compute
Units'' where each unit roughly ``provides the equivalent CPU capacity
of a 1.0--1.2~GHz 2007 Opteron or 2007 Xeon processor,'' according to
Amazon.

Starting from late 2009, AWS also adds to its inventory a set of
instances equipped with GPUs, which is called ``Cluster GPU Quadruple Extra
Large'' (\texttt{cg1.4xlarge}), which is basically a
\texttt{cc1.4xlarge} plus two NVIDIA Tesla ``Fermi'' M2050 GPUs.
%
As of the time of writing, the prices for renting the above compute
resources are shown in Table~\ref{tab:ec2-pricing}.
%
The computation time is always rounded up to the next full hour for
pricing purposes.

\begin{table}
  \begin{center}\vspace*{-2ex}
    \caption{Pricing information from
      \texttt{http://aws.amazon.com/ec2/pricing/}}\vspace*{-3ex}
    \label{tab:ec2-pricing}
    \begin{tabular}{l||c|c|c}
      & Elastic Compute Cloud & 1 Year Reserved Pricing & Elastic MapReduce \\ \hline\hline
      \texttt{cc1.4xlarge} & 1.60 USD/hour & 4290 USD + 0.56 USD/hour & 0.33 USD/hour \\
      \texttt{cg1.4xlarge} & 2.10 USD/hour & 5630 USD + 0.74 USD/hour & 0.42 USD/hour
    \end{tabular}\vspace*{-5ex}
  \end{center}
\end{table}

For computations lasting less than 172 days it is cheaper to use
on-demand pricing.  For longer runs, there is an option to ``reserve''
an instance for 1 year (or even 3), which means that the user pays an
up-front cost (see table above) to cut the on-demand cost of these
instances.




\section{Our Contributions}
In this work, we make a practical contribution on several fronts as following:
first, we parallelize and implement the lattice enumeration  using extreme pruning algorithm \cite{GamaNR10} on GPU and in Cloud, and each implementation gets highly parallelize, namely, about 90 \% parallel benefit. In GPU implementation, we base on the open source code \cite{GPUenum,DBLP:conf/africacrypt/HermansSBVP10} and the performance of one GTX 480 is about 12 times faster than one i7 core. 
%
Second, we extrapolate our average-case run times to estimate the run time of our implementation for solving ASVP instances of the SVP Challenge in higher dimensions.
%
Third, we extend the implementation by using multiple GPUs and run it on Amazon's EC2 in order to harness the immense computational power of such cloud services.
%
Fourth, Consequently, we set new records for the SVP challenge in dimensions $114$, $116$, and $120$. The previous record was for dimension $112$.
%
Fifth, we propose a new security level manner - dollars, which is based on the central idea of cloud computing, that is, buying the computation(rent machine). Compared to the traditional manner - dollar-days, which is based on the idea, buying a mount of machine and running searval days, our new manner is much suited for common user and make people understand the security level directly.


As a result, the average ``cost'' of solving ASVP (and breaking
lattice-based cryptosystems) with our implementation can henceforth be
measured directly in U.S.\ dollars, taking Lenstra's \emph{dollarday}
metric~\cite{Lenstra2004KeyLength} to a next level\footnote{Before the
  final version went to press, it is brought to our attention that,
  unbeknownst to us, Kleinjung, Lenstra, Page, and Smart had also
  started to adopt a similar metric in an ePrint
  report~\cite{KleinjungLPS11} dated May 2011.}.
%
That is, the cost will be shown literally as an amount on your
invoice, e.g., the effort in our solving a 120-dimensional instance of
the SVP Challenge translates to a  2300 USD bill from Amazon.

%
Moreover, this new metric is more practical in that the
parallelizability of the algorithm or the parallelization of the
implementation is \emph{explicitly} taken into account, as opposed to
being assumed or unspecified in the dollarday metric.

%--
Needless to say, such a cost should be understood as an upper bound
obtained based on our implementation, which can certainly be improved, e.g.,
by using a better bounding function or better programming.

\textrm{\\}
\noindent {\it Road Map.} 
In Chapter 2, we survey the varant lattice problems and introduce their properties, some related problem and cryptosystem in detail.
In Chapter 3, we introduce the lattice enumeration algorithm include the overview on lattice reduction, standard enumeration, and enumeration using extreme pruning which we used in our implementation.
In Chapter 4, we show our improvments on parallelization of enumeration algorithm and some modification on the algorithm for speeding up.
In Chapter 5, we describe our implementation on GPU and in clouds, repectively.
Finally, the experiment results is shown in chapter 6.
As an addition, we show something might be further studied and make up the open questions in the field so far.
\begin{comment}
\end{comment}

\end{CJK}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 
